# SynthDoc Environment Configuration Template
# Copy this file to .env and fill in your API keys

# =============================================================================
# LLM API Keys - Choose one or more providers
# =============================================================================

# OpenAI API Key
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Claude API Key  
# Get from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Groq API Key (Fast inference)
# Get from: https://console.groq.com/
GROQ_API_KEY=your_groq_api_key_here

# Cohere API Key
# Get from: https://dashboard.cohere.ai/
COHERE_API_KEY=your_cohere_api_key_here

# Google Gemini API Key
# Get from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# =============================================================================
# Azure OpenAI Configuration (if using Azure)
# =============================================================================

# Azure OpenAI API Key
AZURE_API_KEY=your_azure_openai_api_key_here

# Azure OpenAI Endpoint
AZURE_API_BASE=https://your-resource-name.openai.azure.com/

# Azure OpenAI API Version
AZURE_API_VERSION=2023-05-15

# =============================================================================
# HuggingFace Configuration (for dataset uploads)
# =============================================================================

# HuggingFace Token (for uploading datasets)
# Get from: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=your_huggingface_token_here

# =============================================================================
# SynthDoc Configuration
# =============================================================================

# Default LLM Model to use
# Options: gpt-4o-mini, gpt-4o, claude-3-5-sonnet-20241022, groq/llama-3.1-8b-instant, etc.
DEFAULT_LLM_MODEL=gpt-4o-mini

# Default output directory for generated content
DEFAULT_OUTPUT_DIR=./synthdoc_output

# Enable debug logging (true/false)
DEBUG_MODE=false

# =============================================================================
# Advanced Configuration
# =============================================================================

# Maximum tokens for LLM requests
MAX_TOKENS=2048

# Temperature for LLM generation (0.0 to 1.0)
LLM_TEMPERATURE=0.7

# Request timeout in seconds
REQUEST_TIMEOUT=60

# Enable cost tracking (true/false)
ENABLE_COST_TRACKING=true 